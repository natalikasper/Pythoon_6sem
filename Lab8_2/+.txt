import matplotlib.pyplot as plt
import mglearn
import numpy as np
# ЯДЕРНЫЙ МЕТОД ОПОРНЫХ ВЕКТОРОВ (SVM)
#   расшир.метода оп.в-ров, позвол.получ.более сложн.модели, кот не сводятся к постр.простых гиперпл-стей в пр-ве
# метод оп.в-ров - алг.созд линию / гиперпл-сть для раздел.д-х на классы

# ЛИНЕЙНЫЕ МОДЕЛИ И НЕЛИНЕЙНЫЕ ПРИЗНАКИ
# лин.модели наклад.высьма жесткие ограничения, т.к.линии и гиперпл-сти им.огранич.гибкость
# 1 из способов сделать лин.модель более гибкой - добавить новые признаки

# посм.синтетич.набор д-х
from sklearn.datasets import make_blobs
X, y = make_blobs(centers=4, random_state=8)
y = y % 2

mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Признак 0")
plt.ylabel("Признак 1")
plt.show()

# лин.модель классиф.м.опред.т.только с пом.прямой линии
# и не м.дать хорошее кач-во для этого набора д-х
from sklearn.svm import LinearSVC
linear_svm = LinearSVC().fit(X, y)
mglearn.plots.plot_2d_separator(linear_svm, X)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Признак 0")
plt.ylabel("Признак 1")
plt.show()

# расширим набор вх.признаков, добавим
#   в кач-ве нового признака квадрат второго признака
# теперь к.т.д-х представим в виде т.3мерного пр-ва
X_new = np.hstack([X, X[:, 1:] ** 2])
from mpl_toolkits.mplot3d import Axes3D, axes3d
figure = plt.figure()
# визуализируем в 3D
ax = Axes3D(figure, elev=-152, azim=-26)
# сначала размещаем на графике все точки с y == 0, затем с y == 1
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',
           cmap=mglearn.cm2, s=60)
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',
           cmap=mglearn.cm2, s=60)
ax.set_xlabel("признак0")
ax.set_ylabel("признак1")
ax.set_zlabel("признак1 ** 2")
plt.show()

# м.отделить 2 кл(лин.модель + 3мерное пр-во)
# подогон.лин.модель к доп.д-м
linear_svm_3d = LinearSVC().fit(X_new, y)
coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_

# граница принятия решений
figure = plt.figure()
ax = Axes3D(figure, elev=-152, azim=-26)
xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)
yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)
XX, YY = np.meshgrid(xx, yy)
ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]
ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',
           cmap=mglearn.cm2, s=60)
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',
           cmap=mglearn.cm2, s=60)
ax.set_xlabel("признак0")
ax.set_ylabel("признак1")
ax.set_zlabel("признак1 ** 2")
plt.show()

# модель лин.SVM как ф-ция исх.признаков не явл.линю
# это не линия, а скорее эллипс, как м.увидеть на графике
ZZ = YY ** 2
dec = linear_svm_3d.decision_function(np.c_[XX.ravel(),
                                            YY.ravel(),
                                            ZZ.ravel()])
plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],
             cmap=mglearn.cm2, alpha=0.5)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Признак 0")
plt.ylabel("Признак 1")
# граница принятия реш = ф-ция от 2 исх.признаков
plt.show()

# ЯДЕРНЫЙ ТРЮК - вычисл.евклидовы расстояния, чтобы получить расшир.пр-во признаков без факт.их добавл.

# 2 способа поместить д-е в высокоразм.пр-во :
#   *полиномиальное ярдро (вычисл.все возм.комбин.исх.признаков до опред.степ)
#   *ядро RBF (радиальная базисная ф-ция) = оно рассм.все возм.полиномы всех степеней, важность снижается с возрастанием

# в ходе обучения SVM вычисл.важность к.т.обуч.д-х с т.зр.границы между 2 класси.
# для опред.границы прин.реш.важно лишь часть точек обуч.набора - т.кот.лежат на границе между классами - опорные в-ра

# обучим машину оп.в-ров на 2мерном 2классовом наборе д-х
# граница прин.реш.показана черным цветом; опорн.в-ра - т.обвед.шир.к-ром
from sklearn.svm import SVC
X, y = mglearn.tools.make_handcrafted_dataset()
svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)
mglearn.plots.plot_2d_separator(svm, X, eps=.5)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
# размещаем на графике опорные вектора
sv = svm.support_vectors_
sv_labels = svm.dual_coef_.ravel() > 0
mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)
plt.xlabel("Признак 0")
plt.ylabel("Признак 1")
# SVM дает гладкую и нелинейную границу
plt.show()

# НАСТРОЙКА ПАР-РОВ SVM
# пар-р gamma задает степ.близости располож.точек
# пар-р С ограничивает важность к.точки
fig, axes = plt.subplots(3, 3, figsize=(15, 10))
for ax, C in zip(axes, [-1, 0, 3]):
    for a, gamma in zip(ax, range(-1, 2)):
        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)
axes[0, 0].legend(["class 0", "class 1", "sv class 0", "sv class 1"],
                  ncol=4, loc=(.9, 1.2))
# небольш.знач.gamma - т.рассм.как располож.поблизости + гладкие границы принятия реш
# низкое знач.гамма -  модель низкой сложност; выс- сложн.

# мал.С - граница = линия - неправ.классиф.т.не вл.на линию
# больше С - более сильное вляния - граница более изогнута - правильно классиф.
plt.show()

# сравним SVM с RBF-ядром к набору д-х рака
# по умолч  C=1 и gamma=1/n_features
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, random_state=0)
svc = SVC()
svc.fit(X_train, y_train)
print("Правильность на обучающем наборе: {:.2f}".format(svc.score(X_train, y_train)))
print("Правильность на тестовом наборе: {:.2f}".format(svc.score(X_test, y_test)))

# SVM тред.чтобы все признаки были изменены в 1 и том же масштабе,
#   т.к.чувствутилен к найстройкам пар-ров и масштабир.д-х
# посмот.мин и макс.знач.к.признака в лог-пр-ве
plt.plot(X_train.min(axis=0), 'o', label="min")
plt.plot(X_train.max(axis=0), '^', label="max")
plt.legend(loc=4)
plt.xlabel("Индекс признака")
plt.ylabel("Величина признака")
plt.yscale("log")
# признаки им.различн.порядки величин
plt.show()


# ПРЕДВАРИТЕЛЬНАЯ ОБРАБОТКА ДАННЫХ - масштабирование всех признаков до 1 масштаба
# вычисл.мин.знач.для к.пр.обуч.набора
min_on_training = X_train.min(axis=0)
# ширину диап. для к.пр.( max - min ) обуч.набора
range_on_training = (X_train - min_on_training).max(axis=0)
# вычит.мин.знач.и затем делим на ширину диапазона # min =0 и max =1 для к.пр.
X_train_scaled = (X_train - min_on_training) / range_on_training
print("Минимальное значение для каждого признака\n{}".format(X_train_scaled.min(axis=0)))
print("Максимальное значение для каждого признака\n {}".format(X_train_scaled.max(axis=0)))


# используем ТО ЖЕ САМОЕ преобразование для тестового набора
X_test_scaled = (X_test - min_on_training) / range_on_training
svc = SVC()
svc.fit(X_train_scaled, y_train)
print("Правильность на обучающем наборе: {:.3f}".format(svc.score(X_train_scaled, y_train)))
print("Правильность на тестовом наборе: {:.3f}".format(svc.score(X_test_scaled, y_test)))


svc = SVC(C=100)
svc.fit(X_train_scaled, y_train)

print("Правильность на обучающем наборе: {:.3f}".format(svc.score(X_train_scaled, y_train)))
print("Правильность на тестовом наборе: {:.3f}".format(svc.score(X_test_scaled, y_test)))
