import mglearn
import mglearn.plots as mp
import matplotlib.pyplot as plt
import numpy as np

# наивные байесовские классификторы - семейство классиф., кот схожи с лин.моделями
# + обучаются быстрее
# хорошо исп.для больших наборов д-х и высокоразм.д-х
# + эффективны, т.к. они оценивают пар-ры рассматривая к.признак отдельно
# - более низкая обобщающая способность по сравнению с лин.классиф.

# GaussianNB - примен для непрерывн.д-х
# BernoulliNB - бинарных
# MultinomialNB - счетные/дискретные

# классифик.бернули для подсчета ненулевых частот признаков по к.классу
#   есть 4 т.д-х с 4 бин.признаками
#   есть 2 класса 0 и 1
# для класса 0 (1ая и 3я т.д-х) подсчитаем частоты:
#     1ый признак = 0 два раза
#     2ой признак = 1 один раз, и = 1 один раз
from sklearn.linear_model import LinearRegression

X = np.array([[0, 1, 0, 1],
              [1, 0, 1, 1],
              [0, 0, 0, 1],
              [1, 0, 1, 0]])
y = np.array([0, 1, 0, 1])

# подсчет ненулевых эл-тов в к.классе:
counts = {}
for label in np.unique(y):
# итерируем по каждому классу
# подсчитываем ( суммируем ) элементы 1 по признаку
    counts[label] = X[y == label].sum(axis=0)
print("Частоты признаков:\n{}".format(counts))

# 2 другие модели отличаются с т.зр.вычисляемых статистик
#   мултиномиал - приним.в расчет ср.знач.к.признака для к.класса
#   гаусса - запис.ср.знач, а также стандрат.отконение к.признака для к.класса

# для получения прогноза т.д-х сравнивается со статистиками для к.класса и прогноз.наиб.подходящий класс


# ДЕРЕВЬЯ РЕШЕНИЙ
#   это модели, кот исп.для решения задач классиф.и регрессии
#   задают вопросы и выстраив.иерархию правил "если ... то", кот.приводит к рез
mp.plot_animal_tree()
# к.узел дерева - либо вопрос, либо терминальный цзел, кот.сод.ответ
# ребра соед.вышестоящие узлы с нижестоящими
# построили модель, кот.различает 4 класса животных, используя 3 признака
# вместо того, чтобы строить их вручную мы м.построить их с помощью контролируемого обучения
plt.show()

# ПОСТРОЕНИЕ ДЕРЕВЬЕВ РЕШЕНИЙ
# построим дерево решений для набора д-х two_moons, кот.сост.из точек, обознач.маркерами 2 типов
# к.маркар - свой класс
# на к.класс - 75 т.д-х
mglearn.plots.plot_tree_progressive()
plt.show()

# построение дерева решений = построение послед-сти правил (если.. то...), кот.приведет
#   нас к истнному решению кратчайшим путем.
# последовательность правил - тесты
# для непр.д-х: "признак i больше значения a?"
# для построения дерева алг.выбирает все возм.тесты и нах.тот, кот явл.наиб.информативным
#   с т.зр.прогнозирования знач.целевой пер-ной

# ПЕРВЫЙ ВЫБРАННЫЙ ТЕСТ
# разделение набора д-х по горизонтали в т.0,06 дает наиб.полную инфу
#   лучше всего разделяет т.класс 0 от т.класса 1
# верхний узел(корень) - предст.собой весь набор д-х, сост.из 50 класса 0, 50 т - 1
# разделение вып-ся путем тестирования х[1]<=0,06, обознач.черной линией
# если тест верен - точка назнач.левому узлу, кот.сод.2 т.класса 0, и 32 класса 1
# в противном случае - т.присвоена правому узлу, кот.сод.48 т.класса 0, и 18 - к.1

# несмотря на то, что 1ое разбиение довольно хоршоо разделило 2 класса, нижняя обл.
#   по-прежнему сод.т., кот.принадл.к классу 0, а верх.обл.по прежнему сод.т.,принадл к к.1
# мы м.построить более точную модель, повторяя процесс поиска наилучшего теста в обоих обл
# график показ.что след.наиб.информативное разбиение для левой и правой области осн.на x[0]

# этот рекурс.проц.строит в итоге бин.дерево решений, в кот.к.узел соотв.опред.тесту
# рекурс.разбиение д-х повтор.до тех пор, пока все т.д-х в к.обл.разбиения не будут
#   принадл.1 и тому же знач.целевой пер-ной
# лист дерева, кот.сод.т.д-х. кот.относ.к одному и тому же знач.целевой пер-ной - чистый
# построение дерева продолж.до тех пор, пока все листья не станут чистыми
# ИТОГОВОЕ РАЗБИЕНИЕ ДЛЯ НАШЕГО НАБОРА Д-Х

# ПРОНОЗ ДЛЯ НОВОЙ ТОЧКИ
# 1)выясн.в какой обл.разбиения пр-ва признаков нах.т
# 2)опред.класс, к кот.относ большинство точек в этой обл
# обл.мб найдена с пом обхода дерева,начиная с корня и путем перемещения влево/вправо
#   в завис.от того, вып-ся тест или нет

# КАК ПРЕДОТВРАТИТЬ ПЕРЕОБУЧЕНИЕ
# 1) предварительная обрезка - ранняя остановка построения дерева
# 2) пост-обрезка (обрезка) - построение дерева с послед.удалением или сокращением малоинформативных узлов

# ПРЕДВАРИТЕЛЬНАЯ ОБРЕЗКА

# набор д-х - рак
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
# строим модель, исп-я настройки по умол для построения полного дерева)
#   => выращиваем дерево до тех пор, пока все листья не станут чистыми
# зафиксируем random_state для воспроизводимости рез-тов
tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, y_train)
# 100 - т.к.листя чистые
# дерево им.глубину достаточную для того, чтобы запомнить все метки обуч.д-х
print("Правильность на обучающем наборе: {:.3f}".format(tree.score(X_train, y_train)))
# 95 - хуже чем на обуч
print("Правильность на тестовом наборе: {:.3f}".format(tree.score(X_test, y_test)))
# если не ограничить глубину - дерево мб сколь угодно глубоким и сложным
# => необрезанные листья склонны к переобучению и плохо обобщ.рез-т на новые д-е

# применим предварительную обрезку, кот.остановит процесс построения дерева до того
#   как мы идеально подгоним модель к обуч.д-м
# остановим процесс построения дерева по достижению опред.глубины
# макс.глубина = 4, т.е.м.задать только 4 послед.вопроса => уменьшим переобуч.
tree = DecisionTreeClassifier(max_depth=4, random_state=0)
tree.fit(X_train, y_train)
# более низкая правильность на обуч.наборе, но лучше правильность на тест.
print("Правильность на обучающем наборе: {:.3f}".format(tree.score(X_train, y_train)))
print("Правильность на тестовом наборе: {:.3f}".format(tree.score(X_test, y_test)))

# АНАЛИЗ ДЕРЕВЬЕВ РЕШЕНИЙ
# м.визуализ.дерево, исп-я ф-цию export_graphviz из модуля tree
# она запис.файл в формате .dot, кот.явл.форматом тестового файля для опис.графиков
# м.задать цвет узлам, чтобы выделить класс, кот.набрал большинство в к.узле
#   и передать имена классов и признаков, чтобы дерево было правильно размечено
from sklearn.tree import export_graphviz
export_graphviz(tree, out_file="tree.dot", class_names=["malignant", "benign"],
                feature_names=cancer.feature_names, impurity=False, filled=True)

# м.прочитать этот файл и визуализ.его, исп-я модуль graphwiz
import graphviz
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)

# ВАЖНОСТЬ ПРИЗНАКОВ В ДЕРЕВЬЯХ
# вместо того, чтобы просм.все дерево, есть нек.показатели кот.мы м.исп.как итогововые показатели работы дерева
# наиб.часто исп-мый показатель - важность признаков, кот.оценивает насколько важен к.признак с т.зр.получ.решений
# это число варьирует в диап.от 0 до 1 для к.признака
#   0 - не исп.вообще
#   1 - отлично предск.целевую пер-ную
# все признаки в сумме = 1
print("Важности признаков:\n{}".format(tree.feature_importances_))

# приведенная сводка не совсем удобна, т.к.мы не знаем каким именно признакам соотв.
#   приведенные важности
# чтобы исправить это:
for name, score in zip(cancer["feature_names"], tree.feature_importances_):
    print(name, score)

# м.визуализир.важности признаков аналогично тому, как мы визуализ.коэф.лин.модели
def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel("Важность признака")
    plt.ylabel("Признак")
plot_feature_importances_cancer(tree)
plt.show()

# деревья регресии не ум.экстраполировать,т.е.делать прогнозы вне диапазона знач.обуч.д-х
# рассм.это с пом.набора д-х RAMPrice, кот.сод.д-е о ценах на комп.память
# ось х - дата
# ось у - цена 1 мбайта ОП
import pandas as pd
ram_prices = pd.read_csv("F:/ram_price.csv")
plt.semilogy(ram_prices.date, ram_prices.price)
plt.xlabel("Год")
plt.ylabel("Цена $/Мбайт")
plt.show()

# сравним DecisionTreeRegressor и LinearRegression
# отмасшабируем цены, используя логарифм => взаимосвязь будет относительно линейной
# несущественно для (1), но существенно для (2)
from sklearn.tree import DecisionTreeRegressor
# используем исторические данные для прогнозирования цен после 2000 года
data_train = ram_prices[ram_prices.date < 2000]
data_test = ram_prices[ram_prices.date >= 2000]
# прогнозируем цены по датам
X_train = data_train.date[:, np.newaxis]
# мы исп.логпреобразование , что получить простую взаимос.между данными и откликом
y_train = np.log(data_train.price)
tree = DecisionTreeRegressor().fit(X_train, y_train)
linear_reg = LinearRegression().fit(X_train, y_train)
# прогнозируем по всем данным
X_all = ram_prices.date[:, np.newaxis]
pred_tree = tree.predict(X_all)
pred_lr = linear_reg.predict(X_all)
# экспоне н цируем, чтобы обратить логарифмическое преобразование
price_tree = np.exp(pred_tree)
price_lr = np.exp(pred_lr)

# сравниваем прогнозы дерева реений и линейной регрессии с реальными
plt.semilogy(data_train.date, data_train.price, label="Обучающие данные")
plt.semilogy(data_test.date, data_test.price, label="Тестовые данные")
plt.semilogy(ram_prices.date, price_tree, label="Прогнозы дерева")
plt.semilogy(ram_prices.date, price_lr, label="Прогнозы линейной регрессии")
plt.legend()
# разница между моделя - впечатляющая
# лин.модель апроксимирует д-е с пом.уже изв.прямой линии
#   эта линия дает достаточно хороший прогноз для тест.д-х, сглаживая всплески на обуч.и тест.д-х
# модель дерева отлично прогноз.на обуч.д-х
#   однако, когда мы вых.из диап.знач.модель просто продолж.предск.посл.изв.т.
#   дерево не способно генер.новые ответы, кот.вых.за пределеы знач.обуч.д-х
plt.show()
